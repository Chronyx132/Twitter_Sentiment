{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re\n",
    "import heapq as heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MultiLabelBinarizer, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import random as random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('CleanedTweets.csv')\n",
    "tweets['processed_text'] = tweets['processed_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>emoticons</th>\n",
       "      <th>ellipsis</th>\n",
       "      <th>word_count</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>- awww, that's a bummer.  you shoulda got da...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>- awww , bummer . shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>is upset can't update facebook texting ... mig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>dived many time ball . managed save 50 % rest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>no , not behaving . mad . ? can't see .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentiment                                               text  \\\n",
       "0           0          0    - awww, that's a bummer.  you shoulda got da...   \n",
       "1           1          0  is upset that he can't update his facebook by ...   \n",
       "2           2          0   i dived many times for the ball. managed to s...   \n",
       "3           3          0    my whole body feels itchy and like its on fire    \n",
       "4           4          0   no, it's not behaving at all. i'm mad. why am...   \n",
       "\n",
       "   mentions  hashtags  urls  exclamations  emoticons  ellipsis  word_count  \\\n",
       "0         1         0     1             0          1         0          19   \n",
       "1         0         0     0             1          0         1          21   \n",
       "2         1         0     0             0          0         0          18   \n",
       "3         0         0     0             0          0         0          10   \n",
       "4         1         0     0             0          0         0          21   \n",
       "\n",
       "                                      processed_text  \n",
       "0  - awww , bummer . shoulda got david carr third...  \n",
       "1  is upset can't update facebook texting ... mig...  \n",
       "2  dived many time ball . managed save 50 % rest ...  \n",
       "3                    whole body feel itchy like fire  \n",
       "4            no , not behaving . mad . ? can't see .  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bow(X_train, X_test, ngram_range=(1, 1), max_features=5000):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range\n",
    "    )\n",
    "    \n",
    "    X_train_bow = vectorizer.fit_transform(X_train['processed_text'])\n",
    "    X_test_bow = vectorizer.transform(X_test['processed_text'])\n",
    "    \n",
    "    return X_train_bow, X_test_bow, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tfidf(X_train, X_test, ngram_range=(1, 1), max_features=5000):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train['processed_text'])\n",
    "    X_test_tfidf = vectorizer.transform(X_test['processed_text'])\n",
    "\n",
    "    return X_train_tfidf, X_test_tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADERS Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "def encode_vader(X_train, X_test):\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def extract_vader_scores(texts):\n",
    "        compound_scores = []\n",
    "        positive_scores = []\n",
    "        negative_scores = []\n",
    "        neutral_scores = []\n",
    "        \n",
    "        for text in texts:\n",
    "            scores = vader.polarity_scores(str(text))\n",
    "            compound_scores.append(scores['compound'])\n",
    "            positive_scores.append(scores['pos'])\n",
    "            negative_scores.append(scores['neg'])\n",
    "            neutral_scores.append(scores['neu'])\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'vader_compound': compound_scores,\n",
    "            'vader_positive': positive_scores,\n",
    "            'vader_negative': negative_scores,\n",
    "            'vader_neutral': neutral_scores\n",
    "        })\n",
    "    \n",
    "    X_train_vader = extract_vader_scores(X_train['text'])\n",
    "    X_test_vader = extract_vader_scores(X_test['text'])\n",
    "    \n",
    "    return X_train_vader, X_test_vader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets.drop(columns = ['sentiment'])\n",
    "y = tweets['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cleaned DataSet and encoding them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fold_datasets(X_fold_train, X_fold_val):\n",
    "    # Get original features\n",
    "    feature_cols = ['mentions', 'hashtags', 'urls', 'exclamations', 'emoticons', 'ellipsis']\n",
    "    X_fold_train_features = X_fold_train[feature_cols].copy()\n",
    "    X_fold_val_features = X_fold_val[feature_cols].copy()\n",
    "    \n",
    "    # Create datasets dictionary\n",
    "    fold_datasets = {}\n",
    "    \n",
    "    # 1. Original features only\n",
    "    fold_datasets['original'] = (X_fold_train_features, X_fold_val_features)\n",
    "    \n",
    "    # 2. BOW features only (Unigram and Bigrams)\n",
    "    bow_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X_fold_train_bow = bow_vectorizer.fit_transform(X_fold_train['processed_text'])\n",
    "    X_fold_val_bow = bow_vectorizer.transform(X_fold_val['processed_text'])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    bow_train_df = pd.DataFrame(\n",
    "        X_fold_train_bow.toarray(),\n",
    "        columns=[f'bow_{i}' for i in range(X_fold_train_bow.shape[1])]\n",
    "    )\n",
    "    bow_val_df = pd.DataFrame(\n",
    "        X_fold_val_bow.toarray(),\n",
    "        columns=[f'bow_{i}' for i in range(X_fold_train_bow.shape[1])]\n",
    "    )\n",
    "    \n",
    "    fold_datasets['bow_only'] = (bow_train_df, bow_val_df)\n",
    "    \n",
    "    # 3. TF-IDF features only (Unigram and Bigrams)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X_fold_train_tfidf = tfidf_vectorizer.fit_transform(X_fold_train['processed_text'])\n",
    "    X_fold_val_tfidf = tfidf_vectorizer.transform(X_fold_val['processed_text'])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    tfidf_train_df = pd.DataFrame(\n",
    "        X_fold_train_tfidf.toarray(),\n",
    "        columns=[f'tfidf_{i}' for i in range(X_fold_train_tfidf.shape[1])]\n",
    "    )\n",
    "    tfidf_val_df = pd.DataFrame(\n",
    "        X_fold_val_tfidf.toarray(),\n",
    "        columns=[f'tfidf_{i}' for i in range(X_fold_train_tfidf.shape[1])]\n",
    "    )\n",
    "    \n",
    "    fold_datasets['tfidf_only'] = (tfidf_train_df, tfidf_val_df)\n",
    "    \n",
    "    # 4. VADER features only\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def get_vader_scores(texts):\n",
    "        scores_df = pd.DataFrame()\n",
    "        scores_df['vader_compound'] = [vader.polarity_scores(str(t))['compound'] for t in texts]\n",
    "        scores_df['vader_pos'] = [vader.polarity_scores(str(t))['pos'] for t in texts]\n",
    "        scores_df['vader_neg'] = [vader.polarity_scores(str(t))['neg'] for t in texts]\n",
    "        scores_df['vader_neu'] = [vader.polarity_scores(str(t))['neu'] for t in texts]\n",
    "        return scores_df\n",
    "    \n",
    "    X_fold_train_vader = get_vader_scores(X_fold_train['text'])\n",
    "    X_fold_val_vader = get_vader_scores(X_fold_val['text'])\n",
    "    \n",
    "    fold_datasets['vader_only'] = (X_fold_train_vader, X_fold_val_vader)\n",
    "    \n",
    "    # 5. Combined datasets\n",
    "    # BOW + original\n",
    "    X_fold_train_bow_combined = pd.concat([X_fold_train_features.reset_index(drop=True), \n",
    "                                         bow_train_df.reset_index(drop=True)], axis=1)\n",
    "    X_fold_val_bow_combined = pd.concat([X_fold_val_features.reset_index(drop=True), \n",
    "                                       bow_val_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    fold_datasets['bow_combined'] = (X_fold_train_bow_combined, X_fold_val_bow_combined)\n",
    "    \n",
    "    # TF-IDF + original\n",
    "    X_fold_train_tfidf_combined = pd.concat([X_fold_train_features.reset_index(drop=True), \n",
    "                                           tfidf_train_df.reset_index(drop=True)], axis=1)\n",
    "    X_fold_val_tfidf_combined = pd.concat([X_fold_val_features.reset_index(drop=True), \n",
    "                                         tfidf_val_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    fold_datasets['tfidf_combined'] = (X_fold_train_tfidf_combined, X_fold_val_tfidf_combined)\n",
    "    \n",
    "    # VADER + original\n",
    "    X_fold_train_vader_combined = pd.concat([X_fold_train_features.reset_index(drop=True), \n",
    "                                           X_fold_train_vader.reset_index(drop=True)], axis=1)\n",
    "    X_fold_val_vader_combined = pd.concat([X_fold_val_features.reset_index(drop=True), \n",
    "                                         X_fold_val_vader.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    fold_datasets['vader_combined'] = (X_fold_train_vader_combined, X_fold_val_vader_combined)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Return all datasets for this fold\n",
    "    return fold_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_with_kfold(X, y, n_folds=5, n_estimator = 100):\n",
    "    # Dictionary to store all results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Create KFold object\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # List of datasets to evaluate\n",
    "    dataset_names = [\n",
    "        'original',          # Just original features\n",
    "        'bow_only',          # Just BOW features\n",
    "        'tfidf_only',        # Just TF-IDF features\n",
    "        'vader_only',        # Just VADER features\n",
    "        'bow_combined',      # BOW + original features\n",
    "        'tfidf_combined',    # TF-IDF + original features\n",
    "        'vader_combined'     # VADER + original features\n",
    "    ]\n",
    "\n",
    "    for name in dataset_names:\n",
    "        all_results[name] = {\n",
    "            'dataset': name,\n",
    "            'cv_scores': [],\n",
    "            'all_predictions': np.array([]),\n",
    "            'all_true': np.array([])\n",
    "        }\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"\\nProcessing fold {fold+1}/{n_folds}...\")\n",
    "        \n",
    "        # Split the data\n",
    "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Generate DataSet for each fold to prevent leakage between train and test\n",
    "        fold_datasets = create_fold_datasets(X_fold_train, X_fold_val)\n",
    "\n",
    "        # Process each dataset type\n",
    "        for dataset_name in dataset_names:\n",
    "            print(f\"  Training on {dataset_name}...\")\n",
    "\n",
    "            X_fold_train_data, X_fold_val_data = fold_datasets[dataset_name]\n",
    "            \n",
    "            # Train Random Forest on this fold and dataset\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimator, random_state=42)\n",
    "            \n",
    "            rf.fit(X_fold_train_data, y_fold_train)\n",
    "\n",
    "            \n",
    "            # Make predictions on validation fold\n",
    "            fold_preds = rf.predict(X_fold_val_data)\n",
    "            fold_score = accuracy_score(y_fold_val, fold_preds)\n",
    "            \n",
    "            # Store results for this dataset type\n",
    "            all_results[dataset_name]['acc_scores'].append(fold_score)\n",
    "            all_results[dataset_name]['all_predictions'] = np.append(\n",
    "                all_results[dataset_name]['all_predictions'], fold_preds\n",
    "            )\n",
    "            all_results[dataset_name]['all_true'] = np.append(\n",
    "                all_results[dataset_name]['all_true'], y_fold_val\n",
    "            )\n",
    "            \n",
    "            # Feature importance\n",
    "            if hasattr(rf, 'feature_importances_') and fold == 0:\n",
    "                importances = rf.feature_importances_\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                features = X_fold_train_data.columns\n",
    "                \n",
    "                # Store top features\n",
    "                top_features = []\n",
    "                for i in range(min(10, len(features))):\n",
    "                    feature_idx = indices[i]\n",
    "                    top_features.append((features[feature_idx], importances[feature_idx]))\n",
    "                \n",
    "                all_results[dataset_name]['top_features'] = top_features\n",
    "            \n",
    "\n",
    "    \n",
    "    # Calculate final metrics for each dataset type\n",
    "    for name in dataset_names:\n",
    "        # Calculate mean\n",
    "        all_results[name]['mean_accuracy'] = np.mean(all_results[name]['acc_scores'])\n",
    "        all_results[name]['std_accuracy'] = np.std(all_results[name]['acc_scores'])\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        all_results[name]['confusion_matrix'] = confusion_matrix(\n",
    "            all_results[name]['all_true'], all_results[name]['all_predictions']\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n--- Results for {name} ---\")\n",
    "        print(f\"accuracies: {all_results[name]['acc_scores']}\")\n",
    "        print(f\"Mean 5-fold accuracy: {all_results[name]['mean_accuracy']:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_results[name]['all_true'], all_results[name]['all_predictions']))\n",
    "        \n",
    "        # Print top features\n",
    "        if 'top_features' in all_results[name]:\n",
    "            print(\"\\nTop 10 features from first fold:\")\n",
    "            for i, (feature, importance) in enumerate(all_results[name]['top_features'], 1):\n",
    "                print(f\"{i}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing fold 1/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 2/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 3/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 4/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 5/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "--- Results for original ---\n",
      "accuracies: [0.606375, 0.610875, 0.6106875, 0.613375, 0.6105]\n",
      "Mean 5-fold accuracy: 0.6104\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.60      0.60     40000\n",
      "         1.0       0.61      0.62      0.62     40000\n",
      "\n",
      "    accuracy                           0.61     80000\n",
      "   macro avg       0.61      0.61      0.61     80000\n",
      "weighted avg       0.61      0.61      0.61     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. mentions: 0.5966\n",
      "2. exclamations: 0.1974\n",
      "3. urls: 0.1054\n",
      "4. ellipsis: 0.0427\n",
      "5. emoticons: 0.0360\n",
      "6. hashtags: 0.0220\n",
      "\n",
      "--- Results for bow_only ---\n",
      "accuracies: [0.749125, 0.75175, 0.747625, 0.7484375, 0.755]\n",
      "Mean 5-fold accuracy: 0.7504\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.77      0.76     40000\n",
      "         1.0       0.76      0.73      0.75     40000\n",
      "\n",
      "    accuracy                           0.75     80000\n",
      "   macro avg       0.75      0.75      0.75     80000\n",
      "weighted avg       0.75      0.75      0.75     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. bow_3685: 0.0100\n",
      "2. bow_4254: 0.0088\n",
      "3. bow_3078: 0.0083\n",
      "4. bow_2829: 0.0081\n",
      "5. bow_4865: 0.0075\n",
      "6. bow_2652: 0.0071\n",
      "7. bow_3032: 0.0067\n",
      "8. bow_1898: 0.0067\n",
      "9. bow_1714: 0.0066\n",
      "10. bow_3858: 0.0048\n",
      "\n",
      "--- Results for tfidf_only ---\n",
      "accuracies: [0.753875, 0.7540625, 0.750625, 0.75575, 0.7601875]\n",
      "Mean 5-fold accuracy: 0.7549\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.77      0.76     40000\n",
      "         1.0       0.76      0.74      0.75     40000\n",
      "\n",
      "    accuracy                           0.75     80000\n",
      "   macro avg       0.76      0.75      0.75     80000\n",
      "weighted avg       0.76      0.75      0.75     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. tfidf_3685: 0.0101\n",
      "2. tfidf_3078: 0.0097\n",
      "3. tfidf_4254: 0.0093\n",
      "4. tfidf_1714: 0.0087\n",
      "5. tfidf_4865: 0.0083\n",
      "6. tfidf_2652: 0.0081\n",
      "7. tfidf_2829: 0.0081\n",
      "8. tfidf_3032: 0.0074\n",
      "9. tfidf_1898: 0.0068\n",
      "10. tfidf_2195: 0.0064\n",
      "\n",
      "--- Results for vader_only ---\n",
      "accuracies: [0.6514375, 0.6525, 0.6413125, 0.64875, 0.6504375]\n",
      "Mean 5-fold accuracy: 0.6489\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.52      0.60     40000\n",
      "         1.0       0.62      0.78      0.69     40000\n",
      "\n",
      "    accuracy                           0.65     80000\n",
      "   macro avg       0.66      0.65      0.64     80000\n",
      "weighted avg       0.66      0.65      0.64     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. vader_compound: 0.4781\n",
      "2. vader_pos: 0.1864\n",
      "3. vader_neg: 0.1762\n",
      "4. vader_neu: 0.1593\n",
      "\n",
      "--- Results for bow_combined ---\n",
      "accuracies: [0.75075, 0.7525, 0.7506875, 0.755375, 0.7576875]\n",
      "Mean 5-fold accuracy: 0.7534\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.76      0.75     40000\n",
      "         1.0       0.76      0.75      0.75     40000\n",
      "\n",
      "    accuracy                           0.75     80000\n",
      "   macro avg       0.75      0.75      0.75     80000\n",
      "weighted avg       0.75      0.75      0.75     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. mentions: 0.0291\n",
      "2. exclamations: 0.0136\n",
      "3. bow_3685: 0.0096\n",
      "4. bow_3078: 0.0079\n",
      "5. bow_2829: 0.0077\n",
      "6. bow_4254: 0.0072\n",
      "7. bow_2652: 0.0067\n",
      "8. bow_4865: 0.0063\n",
      "9. bow_3032: 0.0062\n",
      "10. bow_1714: 0.0062\n",
      "\n",
      "--- Results for tfidf_combined ---\n",
      "accuracies: [0.7563125, 0.7556875, 0.751125, 0.7621875, 0.763125]\n",
      "Mean 5-fold accuracy: 0.7577\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.76      0.76     40000\n",
      "         1.0       0.76      0.75      0.76     40000\n",
      "\n",
      "    accuracy                           0.76     80000\n",
      "   macro avg       0.76      0.76      0.76     80000\n",
      "weighted avg       0.76      0.76      0.76     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. mentions: 0.0277\n",
      "2. exclamations: 0.0116\n",
      "3. tfidf_3685: 0.0096\n",
      "4. tfidf_3078: 0.0094\n",
      "5. tfidf_1714: 0.0084\n",
      "6. tfidf_2829: 0.0080\n",
      "7. tfidf_2652: 0.0077\n",
      "8. tfidf_4865: 0.0076\n",
      "9. tfidf_3032: 0.0073\n",
      "10. tfidf_4254: 0.0073\n",
      "\n",
      "--- Results for vader_combined ---\n",
      "accuracies: [0.681375, 0.6791875, 0.6738125, 0.68125, 0.679625]\n",
      "Mean 5-fold accuracy: 0.6790\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.68      0.68     40000\n",
      "         1.0       0.68      0.68      0.68     40000\n",
      "\n",
      "    accuracy                           0.68     80000\n",
      "   macro avg       0.68      0.68      0.68     80000\n",
      "weighted avg       0.68      0.68      0.68     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. vader_compound: 0.3350\n",
      "2. vader_pos: 0.1795\n",
      "3. vader_neu: 0.1690\n",
      "4. vader_neg: 0.1651\n",
      "5. mentions: 0.0481\n",
      "6. exclamations: 0.0424\n",
      "7. ellipsis: 0.0270\n",
      "8. emoticons: 0.0140\n",
      "9. urls: 0.0125\n",
      "10. hashtags: 0.0073\n"
     ]
    }
   ],
   "source": [
    "# Train RandomForest with 100 estimators\n",
    "\n",
    "n_folds = 5\n",
    "n_estimator = 100\n",
    "results = train_random_forest_with_kfold(X_train, y_train, n_folds, n_estimator = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "tfidf_combined: CV Accuracy = 0.7577 ± 0.0044, \n",
      "tfidf_only: CV Accuracy = 0.7549 ± 0.0031, \n",
      "bow_combined: CV Accuracy = 0.7534 ± 0.0027, \n",
      "bow_only: CV Accuracy = 0.7504 ± 0.0027, \n",
      "vader_combined: CV Accuracy = 0.6790 ± 0.0028, \n",
      "vader_only: CV Accuracy = 0.6489 ± 0.0040, \n",
      "original: CV Accuracy = 0.6104 ± 0.0023, \n",
      "\n",
      "Best dataset: tfidf_combined with CV accuracy 0.7577 ± 0.0044\n"
     ]
    }
   ],
   "source": [
    "# Print summary of all results\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "for dataset_name in sorted(results.keys(), key=lambda k: results[k]['mean_accuracy'], reverse=True):\n",
    "    result = results[dataset_name]\n",
    "    print(f\"{dataset_name}: Accuracy = {result['mean_accuracy']:.4f} ± {result['std_accuracy']:.4f}, \")\n",
    "\n",
    "# Identify best dataset\n",
    "best_dataset = max(results.keys(), key=lambda k: results[k]['mean_accuracy'])\n",
    "print(f\"\\nBest dataset: {best_dataset} with accuracy {results[best_dataset]['mean_accuracy']:.4f} ± \" +\n",
    "        f\"{results[best_dataset]['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing fold 1/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 2/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 3/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 4/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "Processing fold 5/5...\n",
      "  Training on original...\n",
      "  Training on bow_only...\n",
      "  Training on tfidf_only...\n",
      "  Training on vader_only...\n",
      "  Training on bow_combined...\n",
      "  Training on tfidf_combined...\n",
      "  Training on vader_combined...\n",
      "\n",
      "--- Results for original ---\n",
      "accuracies: [0.6064375, 0.6111875, 0.610125, 0.6130625, 0.6104375]\n",
      "Mean 5-fold accuracy: 0.6102\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.60      0.60     40000\n",
      "         1.0       0.61      0.63      0.62     40000\n",
      "\n",
      "    accuracy                           0.61     80000\n",
      "   macro avg       0.61      0.61      0.61     80000\n",
      "weighted avg       0.61      0.61      0.61     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. mentions: 0.5990\n",
      "2. exclamations: 0.1986\n",
      "3. urls: 0.1024\n",
      "4. ellipsis: 0.0426\n",
      "5. emoticons: 0.0354\n",
      "6. hashtags: 0.0220\n",
      "\n",
      "--- Results for bow_only ---\n",
      "accuracies: [0.749125, 0.752625, 0.748, 0.7508125, 0.7556875]\n",
      "Mean 5-fold accuracy: 0.7512\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.77      0.76     40000\n",
      "         1.0       0.76      0.73      0.75     40000\n",
      "\n",
      "    accuracy                           0.75     80000\n",
      "   macro avg       0.75      0.75      0.75     80000\n",
      "weighted avg       0.75      0.75      0.75     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. bow_3685: 0.0100\n",
      "2. bow_4254: 0.0090\n",
      "3. bow_3078: 0.0083\n",
      "4. bow_2829: 0.0080\n",
      "5. bow_4865: 0.0075\n",
      "6. bow_2652: 0.0072\n",
      "7. bow_1714: 0.0067\n",
      "8. bow_1898: 0.0066\n",
      "9. bow_3032: 0.0065\n",
      "10. bow_3858: 0.0047\n",
      "\n",
      "--- Results for tfidf_only ---\n",
      "accuracies: [0.7565, 0.7530625, 0.75375, 0.7586875, 0.7621875]\n",
      "Mean 5-fold accuracy: 0.7568\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.77      0.76     40000\n",
      "         1.0       0.77      0.74      0.75     40000\n",
      "\n",
      "    accuracy                           0.76     80000\n",
      "   macro avg       0.76      0.76      0.76     80000\n",
      "weighted avg       0.76      0.76      0.76     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. tfidf_3685: 0.0102\n",
      "2. tfidf_3078: 0.0095\n",
      "3. tfidf_4254: 0.0095\n",
      "4. tfidf_1714: 0.0087\n",
      "5. tfidf_4865: 0.0085\n",
      "6. tfidf_2652: 0.0082\n",
      "7. tfidf_2829: 0.0081\n",
      "8. tfidf_3032: 0.0074\n",
      "9. tfidf_1898: 0.0067\n",
      "10. tfidf_2195: 0.0063\n",
      "\n",
      "--- Results for vader_only ---\n",
      "accuracies: [0.651375, 0.6541875, 0.643125, 0.6506875, 0.651]\n",
      "Mean 5-fold accuracy: 0.6501\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.52      0.60     40000\n",
      "         1.0       0.62      0.78      0.69     40000\n",
      "\n",
      "    accuracy                           0.65     80000\n",
      "   macro avg       0.66      0.65      0.64     80000\n",
      "weighted avg       0.66      0.65      0.64     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. vader_compound: 0.4720\n",
      "2. vader_neg: 0.1907\n",
      "3. vader_pos: 0.1769\n",
      "4. vader_neu: 0.1604\n",
      "\n",
      "--- Results for bow_combined ---\n",
      "accuracies: [0.7541875, 0.7549375, 0.7528125, 0.7575, 0.7618125]\n",
      "Mean 5-fold accuracy: 0.7562\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.76      0.76     40000\n",
      "         1.0       0.76      0.75      0.76     40000\n",
      "\n",
      "    accuracy                           0.76     80000\n",
      "   macro avg       0.76      0.76      0.76     80000\n",
      "weighted avg       0.76      0.76      0.76     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. mentions: 0.0285\n",
      "2. exclamations: 0.0136\n",
      "3. bow_3685: 0.0095\n",
      "4. bow_3078: 0.0080\n",
      "5. bow_2829: 0.0077\n",
      "6. bow_4254: 0.0073\n",
      "7. bow_2652: 0.0066\n",
      "8. bow_4865: 0.0066\n",
      "9. bow_3032: 0.0063\n",
      "10. bow_1714: 0.0063\n",
      "\n",
      "--- Results for tfidf_combined ---\n",
      "accuracies: [0.75875, 0.7555625, 0.75325, 0.7611875, 0.765625]\n",
      "Mean 5-fold accuracy: 0.7589\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.76      0.76     40000\n",
      "         1.0       0.76      0.76      0.76     40000\n",
      "\n",
      "    accuracy                           0.76     80000\n",
      "   macro avg       0.76      0.76      0.76     80000\n",
      "weighted avg       0.76      0.76      0.76     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. mentions: 0.0274\n",
      "2. exclamations: 0.0117\n",
      "3. tfidf_3685: 0.0096\n",
      "4. tfidf_3078: 0.0095\n",
      "5. tfidf_1714: 0.0085\n",
      "6. tfidf_2829: 0.0080\n",
      "7. tfidf_4865: 0.0078\n",
      "8. tfidf_2652: 0.0077\n",
      "9. tfidf_4254: 0.0076\n",
      "10. tfidf_3032: 0.0074\n",
      "\n",
      "--- Results for vader_combined ---\n",
      "accuracies: [0.6825, 0.6815, 0.6749375, 0.6820625, 0.679375]\n",
      "Mean 5-fold accuracy: 0.6801\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.68      0.68     40000\n",
      "         1.0       0.68      0.68      0.68     40000\n",
      "\n",
      "    accuracy                           0.68     80000\n",
      "   macro avg       0.68      0.68      0.68     80000\n",
      "weighted avg       0.68      0.68      0.68     80000\n",
      "\n",
      "\n",
      "Top 10 features from first fold:\n",
      "1. vader_compound: 0.3296\n",
      "2. vader_neg: 0.1757\n",
      "3. vader_pos: 0.1747\n",
      "4. vader_neu: 0.1687\n",
      "5. mentions: 0.0486\n",
      "6. exclamations: 0.0421\n",
      "7. ellipsis: 0.0272\n",
      "8. emoticons: 0.0139\n",
      "9. urls: 0.0122\n",
      "10. hashtags: 0.0073\n"
     ]
    }
   ],
   "source": [
    "# Train RandomForest with 500 estimators\n",
    "\n",
    "n_folds = 5\n",
    "n_estimator = 500\n",
    "results = train_random_forest_with_kfold(X_train, y_train, n_folds, n_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY OF RESULTS ===\n",
      "tfidf_combined: Accuracy = 0.7589 ± 0.0043, \n",
      "tfidf_only: Accuracy = 0.7568 ± 0.0033, \n",
      "bow_combined: Accuracy = 0.7562 ± 0.0032, \n",
      "bow_only: Accuracy = 0.7512 ± 0.0027, \n",
      "vader_combined: Accuracy = 0.6801 ± 0.0028, \n",
      "vader_only: Accuracy = 0.6501 ± 0.0037, \n",
      "original: Accuracy = 0.6102 ± 0.0022, \n",
      "\n",
      "Best dataset: tfidf_combined with accuracy 0.7589 ± 0.0043\n"
     ]
    }
   ],
   "source": [
    "# Print summary of all results\n",
    "print(\"\\n=== SUMMARY OF RESULTS ===\")\n",
    "for dataset_name in sorted(results.keys(), key=lambda k: results[k]['mean_accuracy'], reverse=True):\n",
    "    result = results[dataset_name]\n",
    "    print(f\"{dataset_name}: Accuracy = {result['mean_accuracy']:.4f} ± {result['std_accuracy']:.4f}, \")\n",
    "\n",
    "# Identify best dataset\n",
    "best_dataset = max(results.keys(), key=lambda k: results[k]['mean_accuracy'])\n",
    "print(f\"\\nBest dataset: {best_dataset} with accuracy {results[best_dataset]['mean_accuracy']:.4f} ± \" +\n",
    "        f\"{results[best_dataset]['std_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF would be chosen as the encoding to train with "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
